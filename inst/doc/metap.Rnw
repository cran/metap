\documentclass[12pt]{article}
%\usepackage{amsmath,amssymb}
%\usepackage{mydef2}
\usepackage[round]{natbib}
\usepackage{parskip,url}
% use [numbered] for numbering down to subsection level
%\lhead{}
%\input{title}
%\VignetteIndexEntry{The metap package}
\title{The \pkg{metap} package}
\author{Michael Dewey}
\newcommand{\pkg}[1]{\texttt{#1}}
\newcommand{\func}[1]{\texttt{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\begin{document}
\maketitle

\section{Introduction}

\subsection{What is this document for?}

This document describes some methods for the meta--analysis of
$p$--values
(significance values)
and their implementation
in the package \pkg{metap}.

The problem of meta--analysis of $p$--values is of course
not completely unconnected with the more general issue
of simultaneous statistical inference.

\subsection{Why and when to meta--analyse significance values}

The canonical way to meta--analyse a number of primary studies
is to combine estimates of effect sizes from each of them.
There are a large number of packages for
this purpose available from CRAN
and
described in the task view \url{http://CRAN.R-project.org/view=MetaAnalysis}.
However sometimes
the only available information may be $p$--values
especially when some of the primary
studies were published a long time ago or were published
in sources which were less rigorous about insisting on effect
sizes.
The methods outlined here are designed for this
eventuality.
The situation may also arise that some of the studies
can be combined in a conventional meta--analysis
using effect sizes but there
are many others which cannot and in that case the conventional
meta--analysis of the
subset of studies which do have effect sizes
may usefully be supplemented by an overall
analysis of the $p$--values.

Just for the avoidance of doubt I should point out that
if each study has produced a proportion and the goal is
to synthesise them to a common estimate or analyse the
differences between them then the standard methods
are appropriate not the ones outlined here.
The $p$--values here are significance levels.

\subsection{Notation}

The $k$ studies give rise to $p$--values,
 $p_i, i = 1, \dots, k$.
These are assumed to be independent
We shall also need the ordered $p$--values:
$p_{[1]} \le p_{[2]}, \dots, \le p_{[k]}$
and weights
$w_i, i = 1, \dots, k$.
Logarithms are natural.

\subsection{Preliminaries}

I assume you have installed \textsf{R} and \pkg{metap}.
You then need to load the package.
<<>>=
library(metap)
@
\section{Preparation for meta--analysis of $p$--values}

It is usual to have a directional hypothesis, for
instance that treatment is better than control.
For the methods described here a necessary preliminary
is to ensure that all the $p$--values
refer to the same directional hypothesis.
If the value from the primary study is two--sided it needs to
be converted.
This is not simply a matter of halving the quoted $p$--value
as values in the opposite direction need to be reversed.
A convenience function \func{two2one} is provided for this.
<<>>=
pvals <- c(0.1, 0.1, 0.9, 0.9, 0.9, 0.9)
istwo <- c(TRUE,  FALSE, TRUE, FALSE, TRUE, FALSE)
toinvert <- c(FALSE, TRUE, FALSE, FALSE, TRUE, TRUE)
two2one(pvals, two = istwo, invert = toinvert)
@
Note in particular the way in which $0.9$ is
converted under the different scenarios.

It would be a wise precaution to examine the $p$--values
graphically or otherwise before subjecting them to further analysis.
A function \func{schweder} is provided for this purpose.
This plots the ordered $p$--values, $p_{[i]}$, against
$i$.
Although the original motivation
for the plot is \citet{schweder82}
the function uses a different choice of axes due
to
\citet{benjamini00}.
<<echo = FALSE>>=
data(validity)
@
We will use an example dataset on the validity of student
ratings quoted in \citet{becker94}.
<<>>=
print(validity)
@

\begin{figure}[htbp]
<<fig = TRUE>>=
schweder(validity)
@
\caption{Simple example of plot}
\label{simple}
\end{figure}

\func{schweder} also offers the possibility of drawing one of 
a number of straight line summaries.
The three possible straight line summaries are:
\begin{itemize}
\item
the lowest slope line of Benjaimin and Hochberg drawn by default
as solid,
\item
a least squares line drawn passing through the point
$k+1, 1$
and using a specified fraction of the points
drawn by default as dotted,
\item
a line with user specified
intercept and slope drawn by default as dashed.
\end{itemize}

\begin{figure}[htbp]
<<fig = TRUE>>=
data(validity)
schweder(validity, drawline = c("bh", "ls", "ab"),
   ls.control = list(frac = 0.5), ab.control = list(a = 0, b = 0.01))
@
\caption{Example of plot with lines added}
\label{withlines}
\end{figure}

Another issue is what to do with studies which have simply reported
on whether a conventional level of significance like 0.05
was achieved or not.
If the exact associated $p$ cannot be derived from the
statistics quoted in the primary source then the value
of the level achieved, in this case 0.05, can be used
although this may be conservative.
Studies which simply report not significant could be included
as having $p=1$ (or $p=0.5$
if it is known that the direction was right)
although this is very conservative.

\section{Methods using transformation of the $p$--values}

One class of methods relies on transforming the $p$--values
and then combining them.

\subsection{The method of summation of logs, Fisher's method}

The method relies on the fact that
\begin{equation}
\sum_{i=1}^{k} - 2 \log p_i
\end{equation}
is a chi-squared with $2 k$ df where $k$ is the number
of studies.
Of course the sum of the log of the $p_i$
is also the log of the product of the $p_i$.
Fisher's method is provided in \func{sumlog}.

\subsection{The method of summation of $z$ values, Stouffer's method}

Defined as
\begin{equation}
\frac{\sum_{i=1}^k z(p_i)}{\sqrt{k}}
\end{equation}

is a standard normal deviate where $k$ is the number of studies,
$z()$ is the quantile function of the normal
distribution.

A weighted version is available
\begin{equation}
\frac{\sum_{i=1}^k (w_i z(p_i))}{\sqrt {\sum_{i=1}^k w_i ^ 2}}
\end{equation}

where $w_i$ are the weights.

By default the weights are equal.
In the absence of effect sizes (in which case a method for
combining effect sizes would be more appropriate anyway)
best results are believed to be obtained with weights
proportional to the square root of the sample sizes
\citep{zaykin11}.
The method of summation of $z$
values is provided in \func{sumz}.

\subsection{The method of summation of logits}

Defined as
\begin{equation}
- \frac{\sum_{i=1}^k \log\frac{p}{1 - p}}{C}
\end{equation}
is distributed as Student's $t$ with $5 k + 4$ df
where
\begin{equation}
C = \sqrt\frac{k \pi^2 (5 k + 2)}{3(5 k + 4)}
\end{equation}
and $k$ is the number of studies.
This method is provided in \func{logitp}.
\subsection{Examples}
Using the same example dataset which we have already plotted

<<>>=
sumlog(validity)
sumz(validity)
logitp(validity)
@
\section{Methods using untransformed $p$--values}

\subsection{The method of minimum $p$ and Wilkinson's method}

The minimum $p$ method is usually described in terms
of a rejection at the $\alpha_*$ level of the
null hypothesis
\begin{equation}
p_{[1]} < 1 - (1 - \alpha_*)^{\frac{1}{k}}
\end{equation}

The minimum $p$ method is a special case of Wilkinson's
method which uses $p_{[r]}$ where $1 \le r \le k$
\citep{wilkinson51}.
% p is pbeta(p[r], r, k+1-r)
% critical p is qbeta(alpha, r, k+1-r)
Wilkinson's method is provided in \func{wilkinsonp} and a
convenience function \func{minimump}
with its own \code{print} method is provided for
the minimum $p$ method.
\subsection{The method of summation of $p$--values}

Define
\begin{equation}
S = \sum_{i=1}^k p_i
\end{equation}
then this method is defined as
\begin{equation}
\frac{(S)^k}{k!}
- {k - 1 \choose 1}\frac{(S - 1)^k}{k!}
+ {k - 2 \choose 2}\frac{(S - 2)^k}{k!}
- \dots
\end{equation}

where there are $k$ studies and the series continues until the
term in parentheses in the numerator $(S-i)$
becomes negative \citep{edgington72a}.
This method is provided in \func{sump}.

Some authors use a simpler version,
for instance \citet{rosenthal78} in the text
although compare his Table 4.

\begin{equation}
\frac{(\sum p)^k}{k!}
\end{equation}

where there are $k$ studies
but this can be very conservative when
$\sum p > 1$
There seems no particular need to use this method but
it is returned by \func{sump}
as the value of \code{conservativep}
for use in checking published values.

Note also that there can be numerical problems for extreme values
of $S$ and in that case recourse might be made to
\func{sumz} or \func{logitp} which have similar
properties.

\subsection{The mean $p$ method}

This is defined as
\begin{equation}
z = (0.5 - \bar{p}) \sqrt{12k}
\end{equation}
which is a standard normal \citep{edgington72b}
and
where $\bar p = \frac{\sum_{i=1}^k p_i}{k}$.
\subsection{Examples}
<<>>=
minimump(validity)
sump(validity)
meanp(validity)
@
\section{Other methods}

\subsection{The method of vote--counting}

A simple way of looking at the problem is
vote counting.
If most of the studies have produced results in favour of the
alternative hypothesis irrespective of whether any of them is
individually significant then that might be regarded as evidence
for that alternative.
The numbers for and against may be compared with what
would be expected under the null using the binomial distribution.
A variation on this would allow for a neutral zone of studies
which are considered neither for nor against.
For instance one might only
count studies which have reached some conventional level of
statistical significance in the two different directions.

<<>>=
votep(validity)
@
\section{Comparison of methods}

\subsection{Weighting}

As mentioned above it is possible to weight the $p$--values.
At the moment this is only provided in \func{sumz} as this
is the only method for which a published example is accessible.

\subsection{Directionality}

When the collection of primary studies
contains a number of values significant in both directions
for example four studies having $p$--values
0.001, 0.001, 0.999, 0.999
the methods can give very different results.
If the intention of the synthesis is to examine a directional
hypothesis one would want a method where these cancelled out.
Note that of the methods considered here the method
of the sum of logs and Wilkinson's method (and its special case minimum $p$)
do not cancel out
and report a significant result for this example.
As an example we use \func{sumlog} and \func{sumz}.
<<>>=
pvals <- c(0.001, 0.001, 0.999, 0.999)
sumlog(pvals)
sumz(pvals)
@
Clearly the choice should be made on scientific grounds
not on the baiss of the outcome.

\section{Miscellanea}

The standard \code{print} and \code{plot} methods
are provided.

\begin{table}[htbp]
\begin{tabular}{lccl}
 & \multicolumn{2}{c}{Valid for} \\
 & $p=0$ & $p=1$ & Notes \\
\code{logitp} & N & N \\
\code{meanp} & Y & Y & Requires at least four studies\\
\code{sumlog} & N & Y \\
\code{sump} & Y & Y \\
\code{sumz} & Y & Y \\
\code{votep} & Y & Y \\
\code{wilkinson} & Y & Y \\
\end{tabular}
\caption{Restrictions}
\label{restrictions}
\end{table}
Not all methods work with $p=0$ or $p=1$.
See Table \ref{restrictions} for details.
If these values occur in your dataset
and you do not wish the functions to take
their routine action of excluding that study
then you need to decide what to do.
If you believe that injudicious rounding is to blame
you might wish to replace zero values by the
least upper bound of the
values which would still round to zero to the given number of decimal places.
So you might replace 0.00 with 0.005, 0.000 with 0.0005
and so on.
\section{Feedback}

I aim to include any method for which there exists a published
example against which I can test the code.
I welcome feedback about such sources and any other comments
about either the documentation or the code.
\bibliography{metap}
\bibliographystyle{plainnat}
\end{document}

